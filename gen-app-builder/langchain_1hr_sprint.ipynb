{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ii3ihksojvA"
      },
      "source": [
        "# LangChain 1-hour Sprint\n",
        "\n",
        "![LangChain Logo](https://github.com/rastringer/promptcraft_notebooks/blob/main/images/langchain.png?raw=1)\n",
        "\n",
        "LangChain is a framework for developing applications infused with LLM magic. In this notebook, we will cover some of its most useful and fun features, including:\n",
        "\n",
        "* Templates\n",
        "* Memory\n",
        "* Working with APIs\n",
        "* Chains\n",
        "* Agents\n",
        "* Vector stores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLvQkFXbTd2C"
      },
      "source": [
        "Let's start by importing some packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2k24lCDonBx"
      },
      "outputs": [],
      "source": [
        "! pip install --upgrade google-cloud-aiplatform\n",
        "# LangChain\n",
        "! pip install langchain langchain-experimental langchain[docarray]\n",
        "! pip install pypdf\n",
        "! pip install pydantic==1.10.8\n",
        "# Open source vector store\n",
        "! pip install chromadb==0.3.26\n",
        "! pip install typing-inspect==0.8.0 typing_extensions==4.5.0\n",
        "# For dense vector representations of text\n",
        "! pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPxAFOghonE3"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGVFxl9fjt0I"
      },
      "outputs": [],
      "source": [
        "import vertexai\n",
        "\n",
        "vertexai.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7Y7McwEz8Zk"
      },
      "outputs": [],
      "source": [
        "# Utils\n",
        "import time\n",
        "from typing import List\n",
        "\n",
        "# Langchain\n",
        "import langchain\n",
        "from pydantic import BaseModel\n",
        "\n",
        "print(f\"LangChain version: {langchain.__version__}\")\n",
        "\n",
        "# Vertex AI\n",
        "from google.cloud import aiplatform\n",
        "from langchain.chat_models import ChatVertexAI\n",
        "from langchain.embeddings import VertexAIEmbeddings\n",
        "from langchain.llms import VertexAI\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "\n",
        "print(f\"Vertex AI SDK version: {aiplatform.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LogO0tJ9z-73"
      },
      "outputs": [],
      "source": [
        "# We will use chat for some tasks\n",
        "chat = ChatVertexAI(\n",
        "    max_output_tokens=1024,\n",
        "    temperature=0.2,\n",
        "    top_p=0.8,\n",
        "    top_k=40,\n",
        "    verbose=True)\n",
        "\n",
        "# And we will use the general text llm for others\n",
        "llm = VertexAI(\n",
        "    max_output_tokens=1024,\n",
        "    temperature=0.2,\n",
        "    top_p=0.8,\n",
        "    top_k=40,\n",
        "    verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by3p-b1sJl9C"
      },
      "source": [
        "The simplest LangChain use is to create chats comprising of a `SystemMessage` and `HumanMessage`. This is similar to the `context` and `user_message` that we provide the LLM using the Python client libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lP39CLLa0F2I"
      },
      "outputs": [],
      "source": [
        "chat([HumanMessage(content=\"Hello\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdGjBV420Y6B"
      },
      "outputs": [],
      "source": [
        "res = chat(\n",
        "    [\n",
        "        SystemMessage(\n",
        "            content=\"You are an expert chef that thinks of imaginative recipies when people give you ingredients.\"\n",
        "        ),\n",
        "        HumanMessage(content=\"I have some kidney beans and tomatoes, what would be an easy lunch?\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(res.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiNioRJBeCeU"
      },
      "source": [
        "## Prompt templates\n",
        "\n",
        "Templates are an abstraction that can help keep prompts modular and reusable. This can be especially important in large applications which may require long and varied prompts.\n",
        "\n",
        "Templates may include few-short examples, instructions, or context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSkWubxs1CGM"
      },
      "outputs": [],
      "source": [
        "# The template_string parameters sets the context for the ChatPromptTemplate\n",
        "\n",
        "template_string = \"\"\"Translate the text \\\n",
        "that is delimited by triple backticks \\\n",
        "into a style that is {style}. \\\n",
        "text: ```{text}```\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIlbbFVA0agh"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(template_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upPmDaJTLgw7"
      },
      "source": [
        "The chat prompts are envisioned as a series of messages. Notice the `.messages` and `format_messages` methods in the following cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Yb3dCKwLq2V"
      },
      "outputs": [],
      "source": [
        "# Print out the template\n",
        "prompt_template.messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUeNeCwA1BiE"
      },
      "outputs": [],
      "source": [
        "#Let's check just the prompt\n",
        "prompt_template.messages[0].prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRYnKpuG0-Zz"
      },
      "outputs": [],
      "source": [
        "# Helpful method to keep track of a template's inputs\n",
        "prompt_template.messages[0].prompt.input_variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaD1QsBiKePJ"
      },
      "source": [
        "In this simple example, we translate a customer e-mail into phonetic Glaswegian."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RzYUX3o1G-D"
      },
      "outputs": [],
      "source": [
        "translator_style = \"\"\"A translator that writes in phonetic Glaswegian.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOYeAk6i1Kky"
      },
      "outputs": [],
      "source": [
        "customer_email = \"\"\"\n",
        "This smashing little coffee maker is simply brilliant! \\\n",
        "I'm so pleased with how easy it is to use and how quickly it brews \\\n",
        "a cracking cup of coffee. \\\n",
        "I'm over the moon with this purchase and would highly recommend it \\\n",
        "to any other coffee lover looking for a top-notch brew every time.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwuKkrjL1MP8"
      },
      "outputs": [],
      "source": [
        "# The format_messages method sets up the task specified in the template\n",
        "customer_messages = prompt_template.format_messages(\n",
        "                    style=translator_style,\n",
        "                    text=customer_email)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTqCIvvh1PSz"
      },
      "outputs": [],
      "source": [
        "# Call the LLM to translate to the style of the customer message\n",
        "customer_response = chat(customer_messages)\n",
        "print(customer_response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TE_nyfIgmKO6"
      },
      "source": [
        "## Parsing outputs\n",
        "\n",
        "LangChain makes it easy to return objects from the LLM in a format which we can use for further tasks (for example, adding an item of interest to a shopping cart, or providing a short list back to the LLM for additional questions).\n",
        "\n",
        "Here is an example of parsing customer reviews of a three-course meal in a restaurant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-karEJ_mtBl"
      },
      "outputs": [],
      "source": [
        "customer_review = \"\"\"\\\n",
        "The excellent barbecue cauliflower starter left \\\n",
        "a lasting impression -- gorgeous presentation and flavors, really geared the tastebuds into action. \\\n",
        "Moving on to the main course, pretty great also. \\\n",
        "Delicious and flavorful chickpea and vegetable curry. They really nailed the buttery consistency, \\\n",
        "depth and balance of the spices. \\\n",
        "The dessert was a bit bland. I opted for a vegan chocolate mousse, \\\n",
        "hoping for a decadent and indulgent finale to my meal. \\\n",
        "It was very visually appealing but was missing the smooth, velvety \\\n",
        "texture of a great mousse.\n",
        "\"\"\"\n",
        "\n",
        "review_template = \"\"\"\\\n",
        "For the input text, extract the following details: \\\n",
        "starter: How did the reviewer find the first course? \\\n",
        "Rate either Poor, Good, or Excellent. \\\n",
        "Do the same for the main course and dessert\n",
        "\n",
        "Format the output as JSON with the following keys:\n",
        "starter\n",
        "main_course\n",
        "dessert\n",
        "\n",
        "text: {text}\n",
        "\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cp1dMFxkYd5H"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
        "print(prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHoTT2hrXzRj"
      },
      "outputs": [],
      "source": [
        "messages = prompt_template.format_messages(text=customer_review)\n",
        "response = chat(messages, temperature=0.1)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bV8VHzteZsfG"
      },
      "source": [
        "Though it looks like JSON, our output is actually a string type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fT7RLVX9ZYeg"
      },
      "outputs": [],
      "source": [
        "type(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jr6xxJ2ZxQr"
      },
      "source": [
        "This means we are unable to access values in this fashion (will result in an error):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16sEr4XhZjHS"
      },
      "outputs": [],
      "source": [
        "response.content.get(\"main_course\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YwKaaROZ1xa"
      },
      "source": [
        "This is where LangChain's parser comes in. Here, we import the `ResponseSchema` and `StructuredOutputParser`, which we use to define the format of the results from the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1IPczDvZrJp"
      },
      "outputs": [],
      "source": [
        "from langchain.output_parsers import ResponseSchema\n",
        "from langchain.output_parsers import StructuredOutputParser\n",
        "\n",
        "starter_schema = ResponseSchema(name=\"starter\", description=\"Review of the starter\")\n",
        "main_course_schema = ResponseSchema(name=\"main_course\", description=\"Review of the main course\")\n",
        "dessert_schema = ResponseSchema(name=\"dessert\", description=\"Review of the dessert\")\n",
        "\n",
        "response_schemas = [starter_schema, main_course_schema, dessert_schema]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfkKmhhDb5K2"
      },
      "outputs": [],
      "source": [
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4ztjoM8aXF5"
      },
      "outputs": [],
      "source": [
        "format_instructions = output_parser.get_format_instructions()\n",
        "print(format_instructions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpoJt9EpcfKN"
      },
      "source": [
        "Now we can update our prior review template to include the format instructions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qeh2oItsa5Lu"
      },
      "outputs": [],
      "source": [
        "review_template_2 = \"\"\"\\\n",
        "For the input text, extract the following details: \\\n",
        "starter: How did the reviewer find the first course? \\\n",
        "Rate either Poor, Good, or Excellent. \\\n",
        "Do the same for the main course and dessert\n",
        "\n",
        "starter\n",
        "main_course\n",
        "dessert\n",
        "\n",
        "text: {text}\n",
        "\n",
        "{format_instructions}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
        "\n",
        "messages = prompt.format_messages(text=customer_review,\n",
        "                                format_instructions=format_instructions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O779zcYwYHwV"
      },
      "outputs": [],
      "source": [
        "print(messages[0].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzBUsr3WYK5z"
      },
      "outputs": [],
      "source": [
        "response = chat(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4nq3mM8crLn"
      },
      "source": [
        "Let's try it on the same review\n",
        "\n",
        "Our response starts as an `AIMessage`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4EVPQcQcx-_"
      },
      "outputs": [],
      "source": [
        "type(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbjFhXIXCGZb"
      },
      "source": [
        "Here we parse the `AIMessage` into a Python dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UslUSpVTczoX"
      },
      "outputs": [],
      "source": [
        "output_dict = output_parser.parse(response.content)\n",
        "output_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-g9ngq2DbUJ"
      },
      "source": [
        "Thanks to LangChain's parser, we now have a Python dictionary which we can use for further tasks, for example taking part of the response and using it as an input to another function / process etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWTgsp_rdGq4"
      },
      "outputs": [],
      "source": [
        "type(output_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkEz820ndLGJ"
      },
      "outputs": [],
      "source": [
        "output_dict.get(\"main_course\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MlLBtQI-lCX"
      },
      "source": [
        "## API chains\n",
        "\n",
        "Another of LangChain's useful features is the ability to call external APIs within chains.\n",
        "\n",
        "In this example, we use the `open-meteo.com` API to get weather reports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCOhnssF-tvb"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import APIChain\n",
        "from langchain.chains.api import open_meteo_docs\n",
        "\n",
        "llm = VertexAI(temperature=0)\n",
        "chain = APIChain.from_llm_and_api_docs(\n",
        "    llm,\n",
        "    open_meteo_docs.OPEN_METEO_DOCS,\n",
        "    verbose=True,\n",
        "    limit_to_domains=[\"https://api.open-meteo.com/\"],\n",
        ")\n",
        "chain.run(\n",
        "    \"How is the weather today in Edinburgh, Scotland, in Celsius?\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMao6bOb_vjj"
      },
      "source": [
        "### Wikipedia\n",
        "\n",
        "We can combine the Wikipedia pip package and LangChain's Wikipedia API wrapper get query results from the encyclopedia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaOqZrSk-y24"
      },
      "outputs": [],
      "source": [
        "!pip install wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDASabouAEGS"
      },
      "outputs": [],
      "source": [
        "from langchain.tools import WikipediaQueryRun\n",
        "from langchain.utilities import WikipediaAPIWrapper\n",
        "\n",
        "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
        "\n",
        "wikipedia.run(\"To which bird family does the field sparrow belong?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2g875ERCizg"
      },
      "source": [
        "### Google search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Etx0j9SCAsAM"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMRequestsChain, LLMChain\n",
        "\n",
        "template = \"\"\"Between >>> and <<< are the raw search result text from google.\n",
        "Extract the answer to the question '{query}' or say \"not found\" if the information is not contained.\n",
        "Use the format\n",
        "Extracted:<answer or \"not found\">\n",
        ">>> {requests_result} <<<\n",
        "Extracted:\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    input_variables=[\"query\", \"requests_result\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "\n",
        "chain = LLMRequestsChain(llm_chain=LLMChain(llm=VertexAI(temperature=0), prompt=PROMPT))\n",
        "question = \"What are the official languages in Turkmenistan, and their alphabets?\"\n",
        "inputs = {\n",
        "    \"query\": question,\n",
        "    \"url\": \"https://www.google.com/search?q=\" + question.replace(\" \", \"+\"),\n",
        "}\n",
        "chain(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3ed1-PQJYso"
      },
      "source": [
        "## Memory\n",
        "\n",
        "It is essential that LLMs keep some memory of the prior interactions in a chat to better inform their answers.\n",
        "\n",
        "LangChain offers several approaches and features in this regard. For all details, see the [Memory](https://python.langchain.com/docs/modules/memory/) section of the documentation.\n",
        "\n",
        "### ConversationBufferWindowMemory\n",
        "\n",
        "Maintains a list of the interactions of the conversation over time, using the last K interactions. This can be useful for keeping a sliding window of the most recent interactions, so the buffer does not get too large"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4eQhiiYJbQV"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory\n",
        "\n",
        "memory = ConversationBufferWindowMemory(k=3)\n",
        "\n",
        "memory.save_context({\"input\": \"Hi\"},\n",
        "                    {\"output\": \"How are you?\"})\n",
        "memory.save_context({\"input\": \"Fine thanks\"},\n",
        "                    {\"output\": \"Great\"})\n",
        "\n",
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kN3o4bWVRK23"
      },
      "source": [
        "### ConversationTokenBufferMemory\n",
        "\n",
        "This feature instead keeps a buffer of recent interactions in memory based on token length,  rather than number of interactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvlXfMKXRZwO"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationTokenBufferMemory\n",
        "\n",
        "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=100)\n",
        "memory.save_context({\"input\": \"All alone, she dreams of the stars!\"},\n",
        "                    {\"output\": \"As she should!\"})\n",
        "memory.save_context({\"input\": \"Baking cookies today?\"},\n",
        "                    {\"output\": \"Behold the cookies!\"})\n",
        "memory.save_context({\"input\": \"Chatbots everywhere?\"},\n",
        "                    {\"output\": \"Certainly!\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GwhfKvvRc44"
      },
      "outputs": [],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Nj3OPlKRled"
      },
      "source": [
        "### Conversation summaries\n",
        "\n",
        "LangChain carries forward summaries of chat messages and flushes memory after a specified number of interactions or tokens.\n",
        "\n",
        "Let's first look at using the former, `ConversationBufferWindowMemory`.\n",
        "\n",
        "We set `verbose=True` to show the prompts and information carried forward by the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfSL0M7LPWE3"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "conversation_with_summary = ConversationChain(\n",
        "    llm=VertexAI(temperature=0),\n",
        "    # We set a low k=2, to only keep the last 2 interactions in memory\n",
        "    memory=ConversationBufferWindowMemory(k=2),\n",
        "    verbose=True\n",
        ")\n",
        "conversation_with_summary.predict(input=\"My favourite sport is fencing. Any tips for how I can go pro?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xo2hMT-wPmuQ"
      },
      "outputs": [],
      "source": [
        "conversation_with_summary.predict(input=\"What equipment do I need?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ih9CLZ8AQOY0"
      },
      "outputs": [],
      "source": [
        "conversation_with_summary.predict(input=\"Who are the greats of the sport I can emulate?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8elpkCMvP4qI"
      },
      "outputs": [],
      "source": [
        "# Since we have now passed k=2, the LLM will be unable to answer\n",
        "conversation_with_summary.predict(input=\"What is my favourite sport?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5z_Dhb87R_6q"
      },
      "source": [
        "### ConversationSummaryBufferMemory\n",
        "\n",
        "Ensures conversational memory up to a specified token length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bp56IJU2Q6Ks"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "\n",
        "conversation_with_summary = ConversationChain(\n",
        "    llm=llm,\n",
        "    # Change max_token_limit here after running through the conversation\n",
        "    memory=ConversationTokenBufferMemory(llm=llm, max_token_limit=600),\n",
        "    verbose=True,\n",
        ")\n",
        "conversation_with_summary.predict(input=\"Hi, how are you?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHnnO3MySR6R"
      },
      "outputs": [],
      "source": [
        "conversation_with_summary.predict(input=\"I'm learning the Rust programming language\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7IUBteTSKFo"
      },
      "outputs": [],
      "source": [
        "conversation_with_summary.predict(input=\"What's the best book to help me?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFy53AIISZGp"
      },
      "outputs": [],
      "source": [
        "# Notice the buffer here is updated and clears the earlier exchanges\n",
        "# Depending on how chatty the LLM is feeling, the token limit may have\n",
        "# already been reached, and this cell will yield a generic response.\n",
        "conversation_with_summary.predict(input=\"Wish me luck!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTc4s_mTS5r0"
      },
      "source": [
        "The following cell should generate a reply that is clearly restricted to the general benefits of learning Haskell and missing the previous context of someone trying to learn Rust.\n",
        "\n",
        "Run this cell, then go back to the Keep the conversation going with summaries cell and change `max_token_limit` to 700. Then re-run the entire conversation and notice how the model relates its ouptut about learning Haskell to the context of someone trying to learn Rust."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFg9ajZgSihj"
      },
      "outputs": [],
      "source": [
        "conversation_with_summary.predict(input=\"Would knowing Haskell help me?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipq1hc68ii_T"
      },
      "source": [
        "## Chains\n",
        "\n",
        "Complex applications will require chaining LLMs together, or with other components.\n",
        "\n",
        "We will cover the following types of chains:\n",
        "\n",
        "**Sequential chains**\n",
        "\n",
        "**Router chains**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUcwWDdmi0O1"
      },
      "source": [
        "### LLMChain\n",
        "\n",
        "An LLMChain simply provides a prompt to the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rProHX1izYx"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"What is the best name to describe \\\n",
        "    a company that makes {product}?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAavEr0Ni7Pe"
      },
      "outputs": [],
      "source": [
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "product = \"A saw for laminate wood\"\n",
        "chain.run(product)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8792cV9FjL6C"
      },
      "source": [
        "### Sequential chain\n",
        "\n",
        "A sequential chain makes a series of calls to an LLM. It enables a pipeline-style workflow in which the output from one call becomes the input to the next.\n",
        "\n",
        "The two types include:\n",
        "\n",
        "* `SimpleSequentialChain`, where predictably each step has a single input and output, which becomes the input to the next step.\n",
        "\n",
        "* `SequentialChain`, which allows for multiple inputs and outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwKGeaiJi8uU"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import SimpleSequentialChain\n",
        "from langchain.prompts import PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25X7ALMEjPs-"
      },
      "outputs": [],
      "source": [
        "# This is an LLMChain to write a pitch for a new product\n",
        "# Let's increase the temperature to allow some imagination\n",
        "\n",
        "llm = VertexAI(temperature=0.7)\n",
        "template = \"\"\"You are an entrepreneur. Think of a ground breaking new product and write a short pitch.\n",
        "\n",
        "Title: {title}\n",
        "Entrepreneur: This is a pitch for the above product:\"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"title\"], template=template)\n",
        "pitch_chain = LLMChain(llm=llm, prompt=prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6m7H7JavjVsL"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"You are a panelist on Dragon's Den. Given a \\\n",
        "description of the product, you are to explain why you think it will \\\n",
        "succeed or fail in the market.\n",
        "\n",
        "Product pitch: {pitch}\n",
        "Review by Dragon's Den panelist:\"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"pitch\"], template=template)\n",
        "review_chain = LLMChain(llm=llm, prompt=prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsYZq5BBjpFp"
      },
      "outputs": [],
      "source": [
        "# This is the overall chain where we run these two chains in sequence.\n",
        "overall_chain = SimpleSequentialChain(chains=[pitch_chain, review_chain], verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lI2kt13mjsAx"
      },
      "outputs": [],
      "source": [
        "review = overall_chain.run(\"Portable iced coffee maker\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7LkrG1KjxRv"
      },
      "source": [
        "### Router chain\n",
        "\n",
        "A `RouterChain` dynamically selects the next chain to use for a given input.\n",
        "This feature uses the `MultiPromptChain` to select then answer with the best-suited prompt to the question.\n",
        "\n",
        "This can help a modular architecure, allowing the effective triaging of inputs between relevant prompt templates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6esOD8KCjtY0"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.router import MultiPromptChain\n",
        "\n",
        "korean_template = \"\"\"\n",
        "You are an expert in korean history and culture.\n",
        "Here is a question:\n",
        "{input}\n",
        "\"\"\"\n",
        "\n",
        "spanish_template = \"\"\"\n",
        "You are an expert in spanish history and culture.\n",
        "Here is a question:\n",
        "{input}\n",
        "\"\"\"\n",
        "\n",
        "chinese_template = \"\"\"\n",
        "You are an expert in Chinese history and culture.\n",
        "Here is a question:\n",
        "{input}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqoJDRGkjzRL"
      },
      "outputs": [],
      "source": [
        "prompt_infos = [\n",
        "    {\n",
        "        \"name\": \"korean\",\n",
        "        \"description\": \"Good for answering questions about Korean history and culture\",\n",
        "        \"prompt_template\": korean_template,\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"spanish\",\n",
        "        \"description\": \"Good for answering questions about Spanish history and culture\",\n",
        "        \"prompt_template\": spanish_template,\n",
        "    },\n",
        "     {\n",
        "        \"name\": \"chinese\",\n",
        "        \"description\": \"Good for answering questions about Chinese history and culture\",\n",
        "        \"prompt_template\": chinese_template,\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DalpVKISj3gl"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
        "\n",
        "llm = VertexAI(temperature=0)\n",
        "\n",
        "destination_chains = {}\n",
        "for p_info in prompt_infos:\n",
        "    name = p_info[\"name\"]\n",
        "    prompt_template = p_info[\"prompt_template\"]\n",
        "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    destination_chains[name] = chain\n",
        "\n",
        "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
        "destinations_str = \"\\n\".join(destinations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAKrHbR-kABO"
      },
      "outputs": [],
      "source": [
        "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
        "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZZ1fg0kkADv"
      },
      "outputs": [],
      "source": [
        "# Thanks to Deeplearning.ai for this template and for the\n",
        "# Langchain short course at deeplearning.ai/short-courses/.\n",
        "\n",
        "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
        "language model select the model prompt best suited for the input. \\\n",
        "You will be given the names of the available prompts and a \\\n",
        "description of what the prompt is best suited for. \\\n",
        "You may also revise the original input if you think that revising\\\n",
        "it will ultimately lead to a better response from the language model.\n",
        "\n",
        "<< FORMATTING >>\n",
        "Return a markdown code snippet with a JSON object formatted to look like:\n",
        "```json\n",
        "{{{{\n",
        "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
        "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
        "}}}}\n",
        "```\n",
        "\n",
        "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
        "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
        "well suited for any of the candidate prompts.\n",
        "REMEMBER: \"next_inputs\" can just be the original input \\\n",
        "if you don't think any modifications are needed.\n",
        "\n",
        "<< CANDIDATE PROMPTS >>\n",
        "{destinations}\n",
        "\n",
        "<< INPUT >>\n",
        "{{input}}\n",
        "\n",
        "<< OUTPUT (remember to include the ```json)>>\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlHKi0yMkAGS"
      },
      "outputs": [],
      "source": [
        "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
        "    destinations=destinations_str\n",
        ")\n",
        "router_prompt = PromptTemplate(\n",
        "    template=router_template,\n",
        "    input_variables=[\"input\"],\n",
        "    output_parser=RouterOutputParser(),\n",
        ")\n",
        "\n",
        "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-HucZwnkAII"
      },
      "outputs": [],
      "source": [
        "chain = MultiPromptChain(router_chain=router_chain,\n",
        "                         destination_chains=destination_chains,\n",
        "                         default_chain=default_chain, verbose=True\n",
        "                        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGKSiUOZkMo-"
      },
      "source": [
        "Notice in the outputs the country of speciality is prefixed eg:\n",
        "`chinese: {'input': ...`, denoting the routing to the correct expert."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6H3dVYEkHNw"
      },
      "outputs": [],
      "source": [
        "chain.run(\"What was the Han Dynasty?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMUtRs_BkRG5"
      },
      "outputs": [],
      "source": [
        "chain.run(\"What are some of typical dishes in Catalonia?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZZFI5NWkS-Z"
      },
      "outputs": [],
      "source": [
        "chain.run(\"How would I greet a friend's parents in Korean?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcF_GMsvkU91"
      },
      "outputs": [],
      "source": [
        "chain.run(\"Summarize Don Quixote in a short paragraph\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BudujtydkWUM"
      },
      "outputs": [],
      "source": [
        "# No specialist chain for carburetor advice; this\n",
        "# will be handled as any other input by the foundational model\n",
        "chain.run(\"How can I fix a carburetor?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXe_R6V_kf7G"
      },
      "source": [
        "## Agents and vectorstores\n",
        "\n",
        "This final section of the notebook will cover some of LangChain's most fun and powerful features.\n",
        "\n",
        "Agents have access to tools such as JSON, Wikipedia, Web Search, GitHub or Pandas Dataframes, and can access their capabilities depending on user input.\n",
        "\n",
        "See [here](https://python.langchain.com/docs/integrations/toolkits/) for a full list of agent toolkits.\n",
        "\n",
        "We will work with some data to perform data retrieval using the LLM with embeddings to match customer queries to products. This is known as Retrieval Augmentated Generation, or RAG.\n",
        "\n",
        "We will use the Wayfair [WANDS](https://www.aboutwayfair.com/careers/tech-blog/wayfair-releases-wands-the-largest-and-richest-publicly-available-dataset-for-e-commerce-product-search-relevance) dataset of more than 42,000 products. Here are the steps:\n",
        "\n",
        "* Download the data into a pandas dataframe and take a smaller 1,000-row sample set\n",
        "\n",
        "* Merge then generate embeddings for the product titles and descriptions\n",
        "\n",
        "* Prompt an LLM to retrieve details and relevant documents related to queries.\n",
        "\n",
        "<img src=\"https://assets.wfcdn.com/im/01139917/resize-h800-w800%5Ecompr-r85/2315/231567967/Capricornus+3+Seater+Sofa.jpg\" width=\"250\"/> <img src=\"https://assets.wfcdn.com/im/07725066/resize-h800-w800%5Ecompr-r85/1584/158440119/Vancasso+BOMOOTIUR+Stoneware+Dinnerware+-+Set+of+18.jpg\" width=\"250\"/>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCr6_IMGlUoq"
      },
      "outputs": [],
      "source": [
        "!wget -q https://raw.githubusercontent.com/wayfair/WANDS/main/dataset/product.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qY2cTO5ilY1E"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "product_df = pd.read_csv(\"product.csv\", sep='\\t')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9b6WXptlcsv"
      },
      "source": [
        "We will work with 1,000 items to avoid longer wait times for the embedding and look up processes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ndlo2oNONYxj"
      },
      "outputs": [],
      "source": [
        "product_df = product_df[:2000].dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ihFzIFAlajw"
      },
      "outputs": [],
      "source": [
        "product_df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9w5fj9adNsUM"
      },
      "outputs": [],
      "source": [
        "len(product_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtft6ycVlqbk"
      },
      "outputs": [],
      "source": [
        "# Reduce the df to columns of interest\n",
        "product_df = product_df.filter([\"product_id\", \"product_name\", \"product_description\", \"average_rating\"], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQBJPTihlsb9"
      },
      "outputs": [],
      "source": [
        "product_df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiHIRcVsl0F-"
      },
      "source": [
        "### Import and initialize pandas dataframe agent\n",
        "\n",
        "These tools use the `langchain-experimental` pip package installed at the start of the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "carYxcSOm_WL"
      },
      "source": [
        "### Pandas agent\n",
        "\n",
        "This agent allows us to interact with the dataframe using natural language. LangChain shows us the pandas queries it is composing to answer the questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xP8sJ5TqlzeW"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent\n",
        "from langchain.agents.agent_types import AgentType\n",
        "\n",
        "agent = create_pandas_dataframe_agent(VertexAI(temperature=0), product_df, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDjIU-l8l2N_"
      },
      "outputs": [],
      "source": [
        "agent.run(\"how many rows are there?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HB3GlVLjmsgi"
      },
      "outputs": [],
      "source": [
        "agent.run(\"How many products have a rating of > 4?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8A5C_jQm4UU"
      },
      "source": [
        "### CSV agent\n",
        "\n",
        "We can also work directly on a .csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDxCGtBCnO4M"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame.to_csv(product_df, \"data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-BV7NdvmueF"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.agents.agent_toolkits import create_csv_agent\n",
        "\n",
        "agent = create_csv_agent(\n",
        "    VertexAI(temperature=0),\n",
        "    \"data.csv\",\n",
        "    verbose=True,\n",
        "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnrlNtJemx7J"
      },
      "outputs": [],
      "source": [
        "agent.run(\"How many rows are there?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8GeRRuynd2T"
      },
      "outputs": [],
      "source": [
        "agent.run(\"Do any product descriptions mention cedar wood? Output them as JSON please\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Px8rsmWWng9m"
      },
      "outputs": [],
      "source": [
        "agent.run(\"What is the square root of all ratings for product names featuring sofas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AhOEC8snqs1"
      },
      "source": [
        "### Vector stores\n",
        "\n",
        "We will explore embeddings vectors and vector stores in more detail in [subsequent notebooks](rastringer.io.github.com/promptcraft). Let's see what's possible by concatenating our `product_title` and `product_description` columns and creating a text file from the result. We can then create embeddings and perform various retrieval and Q&A tasks.\n",
        "\n",
        "We will use the open source [Chroma](https://docs.trychroma.com/) vector store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_afza56nyru"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.document_loaders import TextLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6OAprgzPMXu"
      },
      "source": [
        "We will embed a `text_data` column, which will be a concatenation of `product_name` and `product_description`, since both columns provide useful contextual information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDQPwlsUnyt2"
      },
      "outputs": [],
      "source": [
        "product_df['text_data'] = product_df['product_name'] + \" \" + product_df['product_description']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aookNZO4PJE0"
      },
      "outputs": [],
      "source": [
        "product_df[\"text_data\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcXg4duWnywK"
      },
      "outputs": [],
      "source": [
        "# Save the \"text_data\" column to a text file\n",
        "text_file_path = \"combined_text_data.txt\"\n",
        "product_df['text_data'].to_csv(text_file_path, sep='\\t', index=False, header=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgabFckIn4ML"
      },
      "outputs": [],
      "source": [
        "# load the document and split it into chunks\n",
        "loader = TextLoader(\"combined_text_data.txt\")\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwLsDcG-Ph-t"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5__alNyWPiG4"
      },
      "source": [
        "### Text splitter\n",
        "\n",
        "Splitting text is common when working with LangChain and LLMs in general. This practice means we can feed large amounts of data to LLMs for parsing or embedding in chunks, or batches.\n",
        "\n",
        "Ideally, we want to do so in a way that keeps meaningful chunks together. We will use the default recommended `RecursiveCharacterTextSplitter`. We specify a `chunk_size` and `chunk_overlap` to set an upper limit on the size and overlap between the splits / chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-w5bcOjgn5oc"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1500,\n",
        "    chunk_overlap = 150\n",
        ")\n",
        "\n",
        "docs = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUGo_V-Zn6yS"
      },
      "outputs": [],
      "source": [
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBAQIkTEn8MP"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# Clear any previous vector store\n",
        "!rm -rf ./docs/chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZsonVgen9rk"
      },
      "outputs": [],
      "source": [
        "# Takes ~3 mins to run\n",
        "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "db = Chroma.from_documents(docs, embedding_function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLHE2R5kolYN"
      },
      "outputs": [],
      "source": [
        "query = \"Is there a slow cooker?\"\n",
        "docs = db.similarity_search(query, n_results=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePkNsfD_olbH"
      },
      "outputs": [],
      "source": [
        "docs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhmN82xyn-pR"
      },
      "outputs": [],
      "source": [
        "query = \"Recommend a durable door mat\"\n",
        "docs = db.similarity_search(query, n_results=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8uYSe_eRG9K"
      },
      "outputs": [],
      "source": [
        "docs[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNN4-OPjop3Y"
      },
      "source": [
        "### Retrieval\n",
        "\n",
        "A `Retriever` is a method for answering questions based on information in an index.\n",
        "\n",
        "Here, we use `RetrievalQA` this ability with a question and answering chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhaPqMI1opLR"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "llm = VertexAI(\n",
        "    model_name=\"text-bison@001\",\n",
        "    max_output_tokens=1024,\n",
        "    temperature=0.1,\n",
        "    top_p=0.8,\n",
        "    top_k=40,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=db.as_retriever()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIhv6u72ovGo"
      },
      "source": [
        "### Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2LlRGVKopO_"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Build prompt\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end. \\\n",
        "If you don't know the answer, just say that you don't know, \\\n",
        "don't try to make up an answer. Use three sentences maximum. \\\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"],template=template,)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpTMb0zZozCX"
      },
      "outputs": [],
      "source": [
        "# Run chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=db.as_retriever(),\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2rLBHDYozE0"
      },
      "outputs": [],
      "source": [
        "question = \"Can you recommend comfortable bed sheets?\"\n",
        "result = qa_chain({\"query\": question})\n",
        "result[\"result\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5IFVCxbRNqa"
      },
      "outputs": [],
      "source": [
        "question = \"How about a Persian-style rug for my living room.\"\n",
        "result = qa_chain({\"query\": question})\n",
        "result[\"result\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHr7hwltRiMc"
      },
      "source": [
        "## Summary\n",
        "\n",
        "In this whirlwind tour of some of LangChain's features, we covered:\n",
        "\n",
        "* Memory\n",
        "* Chains\n",
        "* Agents\n",
        "* Vector stores\n",
        "\n",
        "LangChain is a fast-evolving project. To explore more features and keep up-to-date with developments, please see the [website](https://www.langchain.com/) or [Python documentation](https://python.langchain.com/docs/get_started/introduction).\n",
        "\n",
        "With thanks to Harrison Chase and the excellent LangChain courses at [deeplearning.ai](https://deeplearning.ai/short-courses)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
